from fastai.vision.all import *

matplotlib.rc('image', cmap='Greys')


path = untar_data(URLs.MNIST_SAMPLE)
path.ls()


t = list((path/'valid'/'3').ls())[0]
tensor(Image.open(t)).shape


train_3 = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'3').ls()]).float() / 255
valid_3 = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float() / 255
train_7 = torch.stack([tensor(Image.open(o)) for o in (path/'train'/'7').ls()]).float() / 255
valid_7 = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float() / 255

train_x = torch.cat((train_3, train_7)).view(-1, 28*28)
valid_x = torch.cat((valid_3, valid_7)).view(-1, 28*28)
train_y = tensor(len(train_3)*[1] + len(train_7)*[0]).unsqueeze(1)
valid_y = tensor(len(valid_3)*[1] + len(valid_7)*[0]).unsqueeze(1)

train_3.shape, valid_3.shape, train_7.shape, valid_7.shape, train_x.shape, valid_x.shape, train_y.shape, valid_y.shape


train_dset = list(zip(train_x, train_y))
train_dset[:1], len(train_dset)


x, y = train_dset[0]
x.shape, y.shape


valid_dset = list(zip(valid_x, valid_y))
valid_dset[:1]


train_dl = DataLoader(train_dset, batch_size=256, shuffle=True)
valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False)

dls = DataLoaders(train_dl, valid_dl)


x, y = first(train_dl)
x.shape, y.shape


def init_params(size, std=1.0):
    return (torch.randn(size)*std).requires_grad_()


weights = init_params((28*28, 1))
bias = init_params(1)


def mnist_loss(preds, targets):
    preds = preds.sigmoid()
    return ((preds-targets)**2).mean()


def linear1(xb): return xb@weights + bias


def batch_accuracy(preds, targets):
    preds = preds.sigmoid()
    return ((preds>0.5)==targets).float().mean()


def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dset]
    return round(torch.stack(accs).mean().item(), 4)


# Manual optimizing
def calc_grad(xb, yb, model):
        preds = model(xb)
        loss = mnist_loss(preds, yb)
        loss.backward()


# An optimizer class (to be used later) that's modelled similar to the fastai optimizer class
class BasicOptimizer:
    def __init__(self, params, lr):
        self.params, self.lr = list(params), lr
        
    def step(self):
        for p in self.params:
            p.data -= p.grad.data * self.lr
            
    def zero(self):
        for p in self.params:
            p.grad = None


params = weights, bias


def train_epoch(model, lr, params):
    for xb, yb in train_dl:
        calc_grad(xb, yb, model)
        for p in params:
            p.data -= p.grad*lr
            p.grad.zero_()


for i in range(20):
    train_epoch(linear1, 1, params)
    print(validate_epoch(linear1), end=" ")


linear_model = nn.Linear(28*28, 1)


optimizer = BasicOptimizer(linear_model.parameters(), 1)


def train_epoch(model):
    for xb, yb in train_dl:
        calc_grad(xb, yb, model)
        optimizer.step()
        optimizer.zero()


def train_model(model, epochs):
    for i in range(epochs):
        train_epoch(model)
        print(validate_epoch(model), end="; ")


train_model(linear_model, 10)


learn = Learner(dls, linear_model, mnist_loss, lr=0.5, metrics=batch_accuracy)
learn.fit(5)


simple_net = nn.Sequential(
    nn.Linear(28*28, 30),
    nn.ReLU(),
    nn.Linear(30, 10),
    nn.ReLU(),
    nn.Linear(10, 1)
)


learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
learn.fit(20, 0.1)


plt.plot(L(learn.recorder.values).itemgot(2));
